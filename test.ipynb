{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.random import random\n",
    "from torch import nn\n",
    "from torch import digamma\n",
    "from torch.distributions import MultivariateNormal as MVN\n",
    "from torch.distributions import Bernoulli as Bern\n",
    "from torch.distributions import Poisson as Pois\n",
    "from torch.distributions import Categorical as Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncollapsedGibbsIBP(nn.Module):\n",
    "    ################################################\n",
    "    ########### UNCOLLAPSED GIBBS SAMPLER ##########\n",
    "    ################################################\n",
    "    ### Depends on a few self parameters but could##\n",
    "    ### be made a standalone script if need be #####\n",
    "    ###############################################\n",
    "    def __init__(self, alpha, K, max_K, sigma_a, sigma_n, epsilon, lambd, phi):\n",
    "        super(UncollapsedGibbsIBP, self).__init__()\n",
    "\n",
    "        # idempotent - all are constant and have requires_grad=False\n",
    "        self.alpha = torch.tensor(alpha)\n",
    "        self.K = torch.tensor(K)\n",
    "        self.max_K = torch.tensor(max_K)\n",
    "        self.sigma_a = torch.tensor(sigma_a)\n",
    "        self.sigma_n = torch.tensor(sigma_n)\n",
    "        self.epsilon = torch.tensor(epsilon)\n",
    "        self.lambd = torch.tensor(lambd)\n",
    "        self.phi = torch.tensor(phi)\n",
    "\n",
    "    def init_A(self,K,D):\n",
    "        '''\n",
    "        Sample from prior p(A_k)\n",
    "        A_k ~ N(0,sigma_A^2 I)\n",
    "        '''\n",
    "        Ak_mean = torch.zeros(D)\n",
    "        Ak_cov = self.sigma_a.pow(2)*torch.eye(D)\n",
    "        p_Ak = MVN(Ak_mean, Ak_cov)\n",
    "        A = torch.zeros(K,D)\n",
    "        for k in range(K):\n",
    "            A[k] = p_Ak.sample()\n",
    "        return A\n",
    "\n",
    "    def init_Y(self, n_filters, n_pixels):\n",
    "        '''\n",
    "        Sample from prior p(Y_kd)\n",
    "        Y_kd ~ Bern(epsilon)\n",
    "        '''\n",
    "        Y = torch.zeros(n_filters,n_pixels)\n",
    "        for k in range(n_filters):\n",
    "            for d in range(n_pixels):\n",
    "                p_Ykd = Bern(self.epsilon)\n",
    "                Y[k,d] = p_Ykd.sample()\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    def left_order_form(self,Z):\n",
    "        Z_numpy = Z.clone().numpy()\n",
    "        twos = np.ones(Z_numpy.shape[0])*2.0\n",
    "        twos[0] = 1.0\n",
    "        powers = np.cumprod(twos)[::-1]\n",
    "        values = np.dot(powers,Z_numpy)\n",
    "        idx = values.argsort()[::-1]\n",
    "        return torch.from_numpy(np.take(Z_numpy,idx,axis=1))\n",
    "\n",
    "    def init_Z(self,N=20):\n",
    "        '''\n",
    "        Samples from the IBP prior that defines P(Z).\n",
    "\n",
    "        First Customer i=1 takes the first Poisson(alpha/(i=1)) dishes\n",
    "        Each next customer i>1 takes each previously sampled dish k\n",
    "        independently with m_k/i where m_k is the number of people who\n",
    "        have already sampled dish k. Z_ik=1 if the ith customer sampled\n",
    "        the kth dish and 0 otherwise.\n",
    "        '''\n",
    "        Z = torch.zeros(N,self.K)\n",
    "        K = int(self.K.item())\n",
    "        total_dishes_sampled = 0\n",
    "        for i in range(N):\n",
    "            selected = torch.rand(total_dishes_sampled) < Z[:,:total_dishes_sampled].sum(dim=0) / (i+1.)\n",
    "            Z[i][:total_dishes_sampled][selected]=1.0\n",
    "            p_new_dishes = Pois(torch.tensor([self.alpha/(i+1)]))\n",
    "            new_dishes = int(p_new_dishes.sample().item())\n",
    "            if total_dishes_sampled + new_dishes >= K:\n",
    "                new_dishes = K - total_dishes_sampled\n",
    "            Z[i][total_dishes_sampled:total_dishes_sampled+new_dishes]=1.0\n",
    "            total_dishes_sampled += new_dishes\n",
    "        \n",
    "        return self.left_order_form(Z)\n",
    "\n",
    "    def remove_allzeros_ZAY(self,Z,A,Y):\n",
    "        \"\"\"\n",
    "        Remove columns (features) from Z that are not active, and also the corresponding rows from A and Y\n",
    "        \"\"\"\n",
    "        to_keep = Z.sum(dim=0) > 0\n",
    "        return Z[:, to_keep], A[to_keep, :], Y[to_keep, :]\n",
    "\n",
    "\n",
    "\n",
    "    def F_loglik_given_ZA(self,F,Z,A):\n",
    "        '''\n",
    "        p(F|Z,A) = 1/([2*pi*sigma_n^2]^(ND/2)) * exp([-1/(2*sigma_n^2)] tr((F-ZA)^T(F-ZA)))\n",
    "        '''\n",
    "        N = F.size()[0]\n",
    "        D = F.size()[1]\n",
    "        pi = np.pi\n",
    "        sig_n2 = self.sigma_n.pow(2)\n",
    "        one = torch.tensor([1.0])\n",
    "        log_first_term = one.log() - (N*D/2.)*(2*pi*sig_n2).log()\n",
    "        log_second_term = ((-1./(2*sig_n2)) * \\\n",
    "            torch.trace((F-Z@A).transpose(0,1)@(F-Z@A)))\n",
    "        log_likelihood = log_first_term + log_second_term\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def X_loglik_given_ZY(self,X,Z,Y):\n",
    "        '''\n",
    "        p(X|Z,Y) = prod_n prod_d p(x_nd|Z,Y)\n",
    "          let e_n = Z_n,:@Y_:,n\n",
    "        p(x_nd=1|Z,Y) = (1 - (1-lamb)^e_n) * (1-epsilon)\n",
    "        p(x_nd=0|Z,Y) = (1-lamb)^e_n * (1-epsilon)\n",
    "        '''\n",
    "        N, D, K = X.shape[0], X.shape[1], Z.shape[1]\n",
    "\n",
    "        lamb = self.lambd\n",
    "        ep = self.epsilon\n",
    "\n",
    "        # Initialize the likelihood variable\n",
    "        log_likelihood = 0.0\n",
    "\n",
    "        # Loop over each image (or do this in a batch-wise fashion)\n",
    "        for i in range(N):\n",
    "            # Compute the effective feature activations for the i-th image\n",
    "            e_n = torch.matmul(Z[i, :], Y) # size (1, D)\n",
    "            \n",
    "            # Calculate the log-likelihood for the i-th image\n",
    "            log_likelihood += torch.sum(\n",
    "                X[i, :] * torch.log(1 - ((1 - lamb) ** e_n) * (1 - ep)) +\n",
    "                (1 - X[i, :]) * torch.log((1 - lamb) ** e_n * (1 - ep))\n",
    "            )\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "\n",
    "    def resample_Z_ik(self,Z,F,X,A,Y,i,k):\n",
    "        '''\n",
    "        m = number of observations not including Z_ik containing feature k\n",
    "\n",
    "        Prior: p(z_ik=1) = m / (N-1)\n",
    "        \n",
    "        Posterior combines the prior with the likelihood:\n",
    "        p(z_ik=1|Z_-nk,F,X,A,Y) propto p(z_ik=1)p(X|Z,Y)p(F|Z,A)\n",
    "        \n",
    "        Z_ik is a Bernoulli RV with this posterior probability\n",
    "        '''\n",
    "        N,D = X.size()\n",
    "        Z_k = Z[:,k]\n",
    "        \n",
    "        m = Z_k.sum() - Z_k[i] # Called m_-nk in the paper\n",
    "\n",
    "        # If Z_nk were 0\n",
    "        Z_if_0 = Z.clone()\n",
    "        Z_if_0[i,k] = 0\n",
    "        \n",
    "        log_prior_if_0 = (1 - (m/(N-1))).log() #Prior\n",
    "        F_log_likelihood_if_0 = self.F_loglik_given_ZA(F,Z_if_0,A) # Likelihood of F\n",
    "        X_log_likelihood_if_0 = self.X_loglik_given_ZY(X,Z_if_0,Y) # Likelihood of X\n",
    "\n",
    "        log_score_if_0 = log_prior_if_0 + F_log_likelihood_if_0 + X_log_likelihood_if_0\n",
    "\n",
    "        # If Z_nk were 1\n",
    "        Z_if_1 = Z.clone()\n",
    "        Z_if_1[i,k]=1\n",
    "        \n",
    "        log_prior_if_1 = (m/(N-1)).log() # Prior\n",
    "        F_log_likelihood_if_1 = self.F_loglik_given_ZA(F,Z_if_1,A) # Likelihood of F  \n",
    "        X_log_likelihood_if_1 = self.X_loglik_given_ZY(X,Z_if_1,Y) # Likelihood of X\n",
    "      \n",
    "        log_score_if_1 = log_prior_if_1 + F_log_likelihood_if_1 + X_log_likelihood_if_1\n",
    "\n",
    "        # Exp, Normalize, Sample\n",
    "        log_scores = torch.cat((log_score_if_0,log_score_if_1),dim=0)\n",
    "        probs = self.renormalize_log_probs(log_scores)\n",
    "        p_znk = Bern(probs[1])\n",
    "\n",
    "        return p_znk.sample() # 0 or 1\n",
    "\n",
    "\n",
    "    def renormalize_log_probs(self,log_probs):\n",
    "        log_probs = log_probs - log_probs.max()\n",
    "        likelihoods = log_probs.exp()\n",
    "        return likelihoods / likelihoods.sum()\n",
    "\n",
    "\n",
    "    def F_loglik_given_k_new(self,cur_F_minus_ZA,Z,D,i,j):\n",
    "        '''\n",
    "        cur_F_minus_ZA is equal to F - ZA, using Z without the\n",
    "        extra j columns that are appended to compute the likelihood\n",
    "        for X|k_new=j. We have to pass this in because Z is changed\n",
    "        in a loop that calls this function.\n",
    "\n",
    "        Z: each time this function is called in the loop one level up,\n",
    "        Z has one more column. Z is N x (K + k_new=j) dimensional.\n",
    "\n",
    "        D: F.size()[1]\n",
    "\n",
    "        i: A few levels up from this function, we are looping through every datapoint,\n",
    "        and for each datapoint, considering how many new features k_new it draws. We\n",
    "        are considering the i^th datapoint.\n",
    "\n",
    "        j: We are calculating the likelihood for F|k_new = j\n",
    "        '''\n",
    "        N,K=Z.size()\n",
    "        cur_F_minus_ZA_T = cur_F_minus_ZA.transpose(0,1)\n",
    "        sig_n = self.sigma_n\n",
    "        sig_a = self.sigma_a\n",
    "\n",
    "        if j==0:\n",
    "            ret = 0.0\n",
    "        else:\n",
    "            w = torch.ones(j,j) + (sig_n/sig_a).pow(2)*torch.eye(j)\n",
    "            # alternative: torch.potrf(a).diag().prod()\n",
    "            w_numpy = w.numpy()\n",
    "            sign,log_det = np.linalg.slogdet(w_numpy)\n",
    "            log_det = torch.tensor([log_det],dtype=torch.float32)\n",
    "            # Note this is in log space\n",
    "            first_term = j*D*(sig_n/sig_a).log() - ((D/2)*log_det)\n",
    "\n",
    "            second_term = 0.5* \\\n",
    "                torch.trace( \\\n",
    "                cur_F_minus_ZA_T @ \\\n",
    "                Z[:,-j:] @ \\\n",
    "                w.inverse() @ \\\n",
    "                Z[:,-j:].transpose(0,1) @ \\\n",
    "                cur_F_minus_ZA) / \\\n",
    "                sig_n.pow(2)\n",
    "            ret = first_term + second_term\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def X_loglik_given_k_new(self, Z, Y, X, i, orig_k, k_new):\n",
    "        \"\"\"\n",
    "        Calculate the log-likelihood of observing X[i, :] given Z, Y, and a proposed new feature count k_new.\n",
    "\n",
    "        Parameters:\n",
    "        - i: int, index of the image row in X\n",
    "        - k_new: int, proposed number of new features\n",
    "        - Z: Tensor, binary matrix of shape (N, K) for current feature ownership\n",
    "        - Y: Tensor, binary matrix of shape (K, d) for feature-to-pixel activations\n",
    "        - X: Tensor, binary matrix of shape (N, d) for observed images\n",
    "        - lamb: float, efficacy parameter for feature activation\n",
    "        - ep: float, spontaneous activation probability for each pixel\n",
    "        - p: float, probability of a new feature turning on a pixel\n",
    "\n",
    "        Returns:\n",
    "        - log_likelihood: Tensor, the computed log-likelihood value for this k_new\n",
    "        \"\"\"\n",
    "\n",
    "        lamb = self.lambd\n",
    "        ep = self.epsilon\n",
    "        p = self.phi\n",
    "\n",
    "        # Compute effective feature activations for the i-th image\n",
    "        e = torch.matmul(Z[i, 0:orig_k], Y[0:orig_k, :])\n",
    "        \n",
    "        # Indices of pixels that are \"on\" and \"off\" in X[i, :]\n",
    "        one_inds = [t for t in range(X.shape[1]) if X[i, t] == 1]\n",
    "        zero_inds = [t for t in range(X.shape[1]) if t not in one_inds]\n",
    "        \n",
    "        # Compute eta values for \"on\" and \"off\" pixels\n",
    "        eta_one = (1 - lamb) ** e[one_inds]\n",
    "        eta_zero = (1 - lamb) ** e[zero_inds]\n",
    "        \n",
    "        # Calculate likelihood components for pixels that are \"on\" and \"off\"\n",
    "        lhood_XiT = torch.sum(torch.log(1 - (1 - ep) * eta_one * ((1 - lamb * p) ** k_new)))\n",
    "        lhood_XiT += torch.sum(torch.log((1 - ep) * eta_zero * ((1 - lamb * p) ** k_new)))\n",
    "        \n",
    "        return lhood_XiT\n",
    "\n",
    "    def sample_k_new(self,Z,F,X,A,Y,i,truncation=10):\n",
    "        '''\n",
    "        i: The loop calling this function is asking this function\n",
    "        \"how many new features (k_new) should data point i draw?\"\n",
    "\n",
    "        truncation: When computing the un-normalized posterior for k_new|X,Z,A, we cannot\n",
    "        compute the posterior for the infinite amount of values k_new could take on. So instead\n",
    "        we compute from 0 up to some high number, truncation, and then normalize. In practice,\n",
    "        the posterior probability for k_new is so low that it underflows past truncation=20.\n",
    "        '''\n",
    "\n",
    "        N,K = Z.size()\n",
    "        D = X.size()[1]\n",
    "\n",
    "        # # Check if we are at the maximum number of features\n",
    "        # if K == self.max_K:\n",
    "        #     return 0\n",
    "\n",
    "        p_k_new = Pois(torch.tensor([self.alpha/N]))\n",
    "        cur_F_minus_ZA = F - Z@A\n",
    "        \n",
    "        prior_poisson_probs = torch.zeros(truncation)\n",
    "        F_log_likelihood = torch.zeros(truncation)\n",
    "        X_log_likelihood = torch.zeros(truncation)\n",
    "\n",
    "        for j in range(truncation):\n",
    "            # Compute the prior probability of k_new equaling j\n",
    "            prior_poisson_probs[j] = p_k_new.log_prob(torch.tensor(j))\n",
    "\n",
    "            # Compute the log likelihood of F with k_new equaling j\n",
    "            F_log_likelihood[j] = self.F_loglik_given_k_new(cur_F_minus_ZA,Z,D,i,j)\n",
    "\n",
    "            # Compute the log likelihood of X with k_new equaling j\n",
    "            X_log_likelihood[j] = self.X_loglik_given_k_new(Z,Y,X,i,K,j)\n",
    "\n",
    "            # Add new column to Z for next feature\n",
    "            zeros = torch.zeros(N)\n",
    "            Z = torch.cat((Z,torch.zeros(N,1)),1)\n",
    "            Z[i][-1]=1\n",
    "\n",
    "        # Compute log posterior of k_new and exp/normalize\n",
    "        log_sample_probs = prior_poisson_probs + F_log_likelihood + X_log_likelihood\n",
    "        sample_probs = self.renormalize_log_probs(log_sample_probs)\n",
    "\n",
    "        # Important: we changed Z for calculating p(k_new| ...) so we must take off the extra rows\n",
    "        Z = Z[:,:-truncation]\n",
    "        assert Z.size()[1] == K\n",
    "        posterior_k_new = Categorical(sample_probs)\n",
    "        return posterior_k_new.sample()\n",
    "\n",
    "    def resample_Z(self,Z,F,X,A,Y):\n",
    "        '''\n",
    "        - Re-samples existing Z_ik by using p(Z_ik=1|Z_-ik,A,X)\n",
    "        - Samples the number of new dishes that customer i takes\n",
    "          corresponding to:\n",
    "            - prior: p(k_new) propto Pois(alpha/N)\n",
    "            - likelihood: p(X|Z_old,A_old,k_new)\n",
    "            - posterior: p(k_new|X,Z_old,A_old)\n",
    "        - Adds the columns to Z corresponding to the new dishes,\n",
    "          setting those columns to 1 for customer i\n",
    "        - Adds rows to A corresponds to the new dishes.\n",
    "          - p(A_new|X,Z_new,Z_old,A_old) propto p(X|Z_new,Z_old,A_old,A_new)p(A_new)\n",
    "        '''\n",
    "\n",
    "        N = F.size()[0]\n",
    "        K = A.size()[0]\n",
    "        \n",
    "        # Iterate over each data point\n",
    "        for i in range(N):\n",
    "            # Resample existing Z_ik\n",
    "            for k in range(K):\n",
    "                Z[i,k] = self.resample_Z_ik(Z,F,X,A,Y,i,k)\n",
    "            \n",
    "            # Decide how many new features to draw\n",
    "            k_new = self.sample_k_new(Z,F,X,A,Y,i)\n",
    "\n",
    "            # Limit such that current_k + k_new <= max_K\n",
    "            # current_k = A.size()[0]\n",
    "            # k_new = np.clip(k_new, 0, self.max_K - current_k)\n",
    "\n",
    "            # If new features are drawn, add them to Z, A, and Y\n",
    "            if k_new > 0:\n",
    "                # Add new columns to Z\n",
    "                Z = torch.cat((Z,torch.zeros(N,k_new)),1)\n",
    "                for j in range(k_new):\n",
    "                    Z[i][-(j+1)] = 1\n",
    "\n",
    "                # Add new rows to A, based on Z and A\n",
    "                A_new = self.A_new(F,k_new,Z,A)\n",
    "                A = torch.cat((A,A_new),dim=0)\n",
    "\n",
    "                # Add new rows to Y, based on Z and Y\n",
    "                Y_new = self.Y_new(k_new,Y.size()[1])\n",
    "                Y = torch.cat((Y,Y_new),dim=0)\n",
    "                # resample Y_new at the new features\n",
    "                Y = self.resample_Y(Z, X, Y, start_idx=K)\n",
    "\n",
    "        return Z, A, Y\n",
    "\n",
    "    def resample_A(self,F,Z):\n",
    "        '''\n",
    "        mu = (Z^T Z + (sigma_n^2 / sigma_A^2) I )^{-1} Z^T  X\n",
    "        Cov = sigma_n^2 (Z^T Z + (sigma_n^2/sigma_A^2) I)^{-1}\n",
    "        p(A|X,Z) = N(mu,cov)\n",
    "        '''\n",
    "        N,D = F.size()\n",
    "        K = Z.size()[1]\n",
    "        ZT = Z.transpose(0,1)\n",
    "        ZTZ = ZT@Z\n",
    "        I = torch.eye(K)\n",
    "        sig_n = self.sigma_n\n",
    "        sig_a = self.sigma_a\n",
    "        mu = (ZTZ + (sig_n/sig_a).pow(2)*I).inverse()@ZT@F\n",
    "        cov = sig_n.pow(2)*(ZTZ + (sig_n/sig_a).pow(2)*I).inverse()\n",
    "        A = torch.zeros(K,D)\n",
    "        for d in range(D):\n",
    "            p_A = MVN(mu[:,d],cov)\n",
    "            A[:,d] = p_A.sample()\n",
    "        return A\n",
    "\n",
    "    def A_new(self,F,k_new,Z,A):\n",
    "        '''\n",
    "        p(A_new | X, Z_new, Z_old, A_old) propto\n",
    "            p(X|Z_new,Z_old,A_old,A_new)p(A_new)\n",
    "        ~ N(mu,cov)\n",
    "            let ones = knew x knew matrix of ones\n",
    "            let sig_n2 = sigma_n^2\n",
    "            let sig_A2 = sigma_A^2\n",
    "            mu =  (ones + sig_n2/sig_a2 I)^{-1} Z_new_T (X - Z_old A_old)\n",
    "            cov = sig_n2 (ones + sig_n2/sig_A2 I)^{-1}\n",
    "        '''\n",
    "        N,D = F.size()\n",
    "        K = Z.size()[1]\n",
    "        assert K == A.size()[0]+k_new\n",
    "        ones = torch.ones(k_new,k_new)\n",
    "        I = torch.eye(k_new)\n",
    "        sig_n = self.sigma_n\n",
    "        sig_a = self.sigma_a\n",
    "        Z_new = Z[:,-k_new:]\n",
    "        Z_old = Z[:,:-k_new]\n",
    "        Z_new_T = Z_new.transpose(0,1)\n",
    "        # mu is k_new x D\n",
    "        mu = (ones + (sig_n/sig_a).pow(2)*I).inverse() @ \\\n",
    "            Z_new_T @ (F - Z_old@A)\n",
    "        # cov is k_new x k_new\n",
    "        cov = sig_n.pow(2) * (ones + (sig_n/sig_a).pow(2)*I).inverse()\n",
    "        A_new = torch.zeros(k_new,D)\n",
    "        for d in range(D):\n",
    "            p_A = MVN(mu[:,d],cov)\n",
    "            A_new[:,d] = p_A.sample()\n",
    "        return A_new\n",
    "\n",
    "    def resample_Y(self, Z, X, Y, start_idx=0):\n",
    "        \"\"\"\n",
    "        Sample the feature-to-pixel activation matrix Y given the current feature matrix Z and the observed images X.\n",
    "        \"\"\"\n",
    "        K = Z.size()[1]\n",
    "        N, T = X.size()\n",
    "        ep = self.epsilon\n",
    "        lamb = self.lambd\n",
    "        p = self.phi\n",
    "\n",
    "        pY_a0 = torch.zeros(K, T)\n",
    "        pY_a1 = torch.zeros(K, T)\n",
    "        \n",
    "        prior_Y_a0 = torch.log(1 - p)\n",
    "        prior_Y_a1 = torch.log(p)\n",
    "\n",
    "        for t in range(T):\n",
    "            for k in range(start_idx, K):\n",
    "                for a in [0, 1]:\n",
    "                    Y[k, t] = a\n",
    "                    e = torch.matmul(Z, Y[:, t])\n",
    "                    \n",
    "                    log_likelihood = torch.sum(\n",
    "                        (X[:, t])*torch.log(1-((1-lamb)**e)*(1-ep)) + (1-X[:, t])*torch.log((1-lamb)**e*(1 - ep))\n",
    "                        )\n",
    "\n",
    "                    if a == 0:\n",
    "                        pY_a0[k, t] = torch.exp(prior_Y_a0 + log_likelihood)\n",
    "                    else:\n",
    "                        pY_a1[k, t] = torch.exp(prior_Y_a1 + log_likelihood)\n",
    "\n",
    "                # Normalize the probabilities\n",
    "                tempsum = pY_a0[k, t] + pY_a1[k, t]\n",
    "                pY_a0[k, t] /= tempsum\n",
    "                pY_a1[k, t] /= tempsum\n",
    "\n",
    "                # Sample the element\n",
    "                p_Ykt = Bern(pY_a1[k, t])\n",
    "                Y[k, t] = p_Ykt.sample()\n",
    "\n",
    "        return Y\n",
    "                \n",
    "                \n",
    "    def Y_new(self, k_new, D):\n",
    "        \n",
    "        return torch.zeros(k_new, D) \n",
    "\n",
    "    def gibbs(self, F, X, iters):\n",
    "        n_obs_X, n_pixels = X.size()\n",
    "        n_obs_F, n_features = F.size()\n",
    "        assert n_obs_X == n_obs_F, \"Number of observations in X and F must match\"\n",
    "        n_obs = n_obs_X\n",
    "\n",
    "        K = self.K\n",
    "\n",
    "        Z = self.init_Z(n_obs)\n",
    "        A = self.init_A(K, n_features)\n",
    "        Y = self.init_Y(K, n_pixels)\n",
    "\n",
    "        As = []\n",
    "        Zs = []\n",
    "        Ys = []\n",
    "\n",
    "        for i in range(iters):\n",
    "            print('iteration:', i, end='\\r')\n",
    "            # Gibbs resampling\n",
    "            A = self.resample_A(F, Z)\n",
    "            Y = self.resample_Y(Z, X, Y)\n",
    "            Z, A, Y = self.resample_Z(Z,F,X,A,Y)\n",
    "\n",
    "            # cleanup\n",
    "            Z, A, Y = self.remove_allzeros_ZAY(Z, A, Y)\n",
    "\n",
    "            # save the samples to the chain\n",
    "            As.append(A.clone().numpy())\n",
    "            Zs.append(Z.clone().numpy())\n",
    "            Ys.append(Y.clone().numpy())\n",
    "\n",
    "        return As,Zs,Ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinations of latent states\n",
    "true_Z_options = [\n",
    "    [1, 0, 0], \n",
    "    [0, 1, 0], \n",
    "    [0, 0, 1], \n",
    "    [1, 1, 0], \n",
    "    [1, 0, 1], \n",
    "    [0, 1, 1], \n",
    "    [1, 1, 1]\n",
    "    ]\n",
    "\n",
    "# True force directions corresponding to each latent state\n",
    "true_A = torch.tensor([\n",
    "    [ 0.2,  0.2], \n",
    "    [-0.2,  0.2], \n",
    "    [   0, -0.2]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "# True observation corresponding to each latent state\n",
    "true_Y = torch.tensor([\n",
    "    [1, 0, 0, 0], \n",
    "    [0, 1, 0, 0], \n",
    "    [0, 0, 0, 1]\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Generate Dataset\n",
    "Z_latent = []\n",
    "X_dataset = []\n",
    "F_dataset = []\n",
    "for i in range(5):\n",
    "    for selected_Z in true_Z_options:\n",
    "        Z_latent.append(selected_Z)\n",
    "\n",
    "        # generate the true F element \n",
    "        true_F = torch.matmul(torch.tensor(selected_Z).float(), true_A)\n",
    "        F_dataset.append(true_F)\n",
    "\n",
    "        # generate the true X as ZY\n",
    "        true_X = torch.matmul(torch.tensor(selected_Z).float(), true_Y)\n",
    "        X_dataset.append(true_X)\n",
    "\n",
    "X_dataset = torch.stack(X_dataset)\n",
    "F_dataset = torch.stack(F_dataset)\n",
    "\n",
    "# print(X_dataset)\n",
    "# print(F_dataset)\n",
    "# print(np.array(Z_latent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise_to_obs(X, F, F_noise_std = 0.01, lambd=0.98, epsilon=0.02):\n",
    "    \"\"\"\n",
    "    Add gaussian noise to force data, and randomly flip pixels in the observation data\n",
    "    \"\"\"\n",
    "    F += torch.randn(F.size()) * F_noise_std\n",
    "    \n",
    "    X_noisy = torch.zeros(X.size())\n",
    "    for i in range(X.size()[0]):\n",
    "        for j in range(X.size()[1]):\n",
    "            if X[i, j] == 1:\n",
    "                X_noisy[i, j] = 1 if random() < lambd else 0\n",
    "            else:\n",
    "                X_noisy[i, j] = 1 if random() < epsilon else 0\n",
    "\n",
    "    return X_noisy, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "X_dataset, F_dataset = add_noise_to_obs(X_dataset, F_dataset)\n",
    "\n",
    "print(X_dataset[:5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 99\r"
     ]
    }
   ],
   "source": [
    "inf = UncollapsedGibbsIBP(alpha=0.05, K=1, max_K=4, sigma_a=0.2, sigma_n=0.1, epsilon=0.01, lambd=0.99, phi=0.25)\n",
    "\n",
    "As, Zs, Ys = inf.gibbs(F_dataset, X_dataset, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mean_from_samples(As, Zs, Ys, n=10):\n",
    "    A_mean = np.round(np.mean(np.array(As[-n:]),axis=0), 2)\n",
    "    Z_mean = np.round(np.mean(np.array(Zs[-n:]),axis=0), 2)\n",
    "    Y_mean = np.round(np.mean(np.array(Ys[-n:]),axis=0), 2)\n",
    "\n",
    "    return A_mean, Z_mean, Y_mean\n",
    "\n",
    "def compare_distance(reference_matrix, inferred_matrix):\n",
    "    \"\"\"\n",
    "    Compare the distance between rows of the reference and the true matrix.\n",
    "    use this to create a permutation matrix that reorders the inffered matrix to match the reference matrix,\n",
    "    and return the permutation matrix\n",
    "    \"\"\"\n",
    "    n, m = reference_matrix.shape\n",
    "    assert inferred_matrix.shape == (n, m)\n",
    "\n",
    "    # compute the distance matrix\n",
    "    distance_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distance_matrix[i, j] = np.linalg.norm(reference_matrix[i] - inferred_matrix[j])\n",
    "\n",
    "    # find the permutation that minimizes the distance\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    row_ind, col_ind = linear_sum_assignment(distance_matrix)\n",
    "    \n",
    "    # create the permutation matrix that corresponds to this reordering\n",
    "    permutation_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        permutation_matrix[i, col_ind[i]] = 1\n",
    "\n",
    "    return permutation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True A:\n",
      "[[ 0.2  0.2]\n",
      " [-0.2  0.2]\n",
      " [ 0.  -0.2]]\n",
      "Inferred A:\n",
      "[[ 0.21  0.19]\n",
      " [-0.2   0.18]\n",
      " [-0.01 -0.17]]\n",
      "\n",
      "True Y:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "Inferred Y:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "\n",
      "True Z:\n",
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [1 1 0]\n",
      " [1 0 1]\n",
      " [0 1 1]\n",
      " [1 1 1]]\n",
      "Inferred Z:\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 1. 0.]\n",
      " [1. 0. 1.]\n",
      " [0. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "A, Z, Y = extract_mean_from_samples(As, Zs, Ys, n=10)\n",
    "\n",
    "reorder = compare_distance(true_A.numpy(), A)\n",
    "\n",
    "print(\"True A:\")\n",
    "print(true_A.numpy())\n",
    "print(\"Inferred A:\")\n",
    "print(np.round(reorder @ A,2))\n",
    "\n",
    "print(\"\\nTrue Y:\")\n",
    "print(true_Y.numpy())\n",
    "print(\"Inferred Y:\")\n",
    "print(np.round(reorder @ Y,2))\n",
    "\n",
    "print(\"\\nTrue Z:\")\n",
    "print(np.array(Z_latent))\n",
    "print(\"Inferred Z:\")\n",
    "print(np.round(Z @ reorder,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
